{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LayoutLMv2 for financial report NER\n",
    "\n",
    "We now come to the training and evaluation of LayoutLMv2 and LayoutXLM.\n",
    "\n",
    "We use the same split that we used for training LayoutLMv1 and load the artifact from W&B for this.\n",
    "\n",
    "Needless to say, that we need to define dataset and `ObjectType`s again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepdoctection as dd\n",
    "from collections import defaultdict\n",
    "import wandb\n",
    "from transformers import LayoutLMTokenizerFast, XLMRobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining `ObjectTypes`, Dataset and Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dd.object_types_registry.register(\"ner_first_page\")\n",
    "class FundsFirstPage(dd.ObjectTypes):\n",
    "\n",
    "    report_date = \"report_date\"\n",
    "    umbrella = \"umbrella\"\n",
    "    report_type = \"report_type\"\n",
    "    fund_name = \"fund_name\"\n",
    "\n",
    "dd.update_all_types_dict()\n",
    "\n",
    "@dd.curry\n",
    "def overwrite_location_and_load(dp, image_dir, load_image):\n",
    "    image_file = image_dir / dp.file_name.replace(\"pdf\",\"png\")\n",
    "    dp.location = image_file.as_posix()\n",
    "    if load_image:\n",
    "        dp.image = dd.load_image_from_file(image_file)\n",
    "    return dp\n",
    "\n",
    "class NerBuilder(dd.DataFlowBaseBuilder):\n",
    "\n",
    "    def build(self, **kwargs) -> dd.DataFlow:\n",
    "        load_image = kwargs.get(\"load_image\", False)\n",
    "        filter_languages = kwargs.get(\"filter_languages\")\n",
    "\n",
    "        ann_files_dir = self.get_workdir()\n",
    "        image_dir = self.get_workdir() / \"image\"\n",
    "\n",
    "        df = dd.SerializerFiles.load(ann_files_dir,\".json\")   # get a stream of .json files\n",
    "        df = dd.MapData(df, dd.Image.from_file)   # load .json file\n",
    "\n",
    "        df = dd.MapData(df, overwrite_location_and_load(image_dir, load_image))\n",
    "\n",
    "        if self.categories.is_filtered():\n",
    "            df = dd.MapData(\n",
    "                df,\n",
    "                dd.filter_cat(\n",
    "                    self.categories.get_categories(as_dict=False, filtered=True),\n",
    "                    self.categories.get_categories(as_dict=False, filtered=False),\n",
    "                ),\n",
    "            )\n",
    "        df = dd.MapData(df,dd.re_assign_cat_ids(cat_to_sub_cat_mapping=self.categories.get_sub_categories(\n",
    "                                                 categories=dd.LayoutType.word,\n",
    "                                                 sub_categories={dd.LayoutType.word: dd.WordType.token_class},\n",
    "                                                 keys = False,\n",
    "                                                 values_as_dict=True,\n",
    "                                                 name_as_key=True)))\n",
    "        \n",
    "        if filter_languages:\n",
    "            df = dd.MapData(df, dd.filter_summary({\"language\": [dd.get_type(lang) for lang in filter_languages]},\n",
    "                                                 mode=\"value\"))\n",
    "\n",
    "        return df\n",
    "    \n",
    "ner = dd.CustomDataset(name = \"FRFPE\",\n",
    "                 dataset_type=dd.DatasetType.token_classification,\n",
    "                 location=\"FRFPE\",\n",
    "                 init_categories=[dd.LayoutType.text, dd.LayoutType.title, dd.LayoutType.list, dd.LayoutType.table,\n",
    "                                  dd.LayoutType.figure, dd.LayoutType.line, dd.LayoutType.word],\n",
    "                 init_sub_categories={dd.LayoutType.word: {dd.WordType.token_class: [FundsFirstPage.report_date,\n",
    "                                                                                     FundsFirstPage.report_type,\n",
    "                                                                                     FundsFirstPage.umbrella,\n",
    "                                                                                     FundsFirstPage.fund_name,\n",
    "                                                                                     dd.TokenClasses.other],\n",
    "                                                           dd.WordType.tag: []}},\n",
    "                 dataflow_builder=NerBuilder)\n",
    "\n",
    "ner.dataflow.categories.filter_categories(categories=dd.LayoutType.word)\n",
    "df = ner.dataflow.build(load_image=True)\n",
    "\n",
    "merge = dd.MergeDataset(ner)\n",
    "merge.explicit_dataflows(df)\n",
    "merge.buffer_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading W&B artifact and building dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"FRFPE_layoutlmv1\", resume=True)\n",
    "artifact = wandb.use_artifact('jm76/FRFPE_layoutlmv1/merge_FRFPE:v0', type='dataset')\n",
    "table = artifact.get(\"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[0608 14:46.11 @base.py:250]\u001b[0m  \u001b[32mINF\u001b[0m  \u001b[97mWill used dataflow from previously explicitly passed configuration\u001b[0m\n",
      "|                                                                                                                                                                                                 |357/?[00:28<00:00,12.63it/s]\n"
     ]
    }
   ],
   "source": [
    "split_dict = defaultdict(list)\n",
    "for row in table.data:\n",
    "    split_dict[row[0]].append(row[1])\n",
    "\n",
    "merge.create_split_by_id(split_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporing the language distribustion across the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train split: {<Languages.english>: 152, <Languages.german>: 145, <Languages.french>: 8}\n",
      "val split: {<Languages.english>: 11, <Languages.german>: 14, <Languages.french>: 1}\n",
      "test split: {<Languages.english>: 17, <Languages.german>: 9, <Languages.french>: 0}\n"
     ]
    }
   ],
   "source": [
    "categories={\"1\": dd.Languages.english, \"2\": dd.Languages.german, \"3\": dd.Languages.french}\n",
    "categories_name_as_key = {val: key for key, val in categories.items()}\n",
    "\n",
    "# train\n",
    "summarizer_train = dd.LabelSummarizer(categories)\n",
    "langs_train = []\n",
    "for dp in merge._dataflow_builder.split_cache[\"train\"]:\n",
    "    langs_train.append(categories_name_as_key[dp.summary.get_sub_category(\"language\").value])\n",
    "summarizer_train.dump(langs_train)\n",
    "   \n",
    "# val\n",
    "summarizer_val = dd.LabelSummarizer(categories)\n",
    "langs_val = []\n",
    "for dp in merge._dataflow_builder.split_cache[\"val\"]:\n",
    "    langs_val.append(categories_name_as_key[dp.summary.get_sub_category(\"language\").value])\n",
    "summarizer_val.dump(langs_val)\n",
    "\n",
    "# test\n",
    "summarizer_test = dd.LabelSummarizer(categories)\n",
    "langs_test = []\n",
    "for dp in merge._dataflow_builder.split_cache[\"test\"]:\n",
    "    langs_test.append(categories_name_as_key[dp.summary.get_sub_category(\"language\").value])\n",
    "summarizer_test.dump(langs_test)\n",
    "\n",
    "train_summary = {categories[key]:val for key, val in summarizer_train.get_summary().items()}\n",
    "val_summary= {categories[key]:val for key, val in summarizer_val.get_summary().items()}\n",
    "test_summary = {categories[key]:val for key, val in summarizer_test.get_summary().items()}\n",
    "\n",
    "print(f\"train split: {train_summary}\")\n",
    "print(f\"val split: {val_summary}\")\n",
    "print(f\"test split: {test_summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language is well balanced across the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.ModelCatalog.get_model_list() # find the model\n",
    "\n",
    "# If you haven't downloaded the base model make sure to have it in your .cache\n",
    "# dd.ModelDownloadManager.maybe_download_weights_and_configs(\"microsoft/layoutlmv2-base-uncased/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_config_json = dd.ModelCatalog.get_full_path_configs(\"microsoft/layoutlmv2-base-uncased/pytorch_model.bin\")\n",
    "path_weights = dd.ModelCatalog.get_full_path_weights(\"microsoft/layoutlmv2-base-uncased/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = dd.get_metric(\"f1\")\n",
    "metric.set_categories(sub_category_names={\"word\": [\"token_class\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.train_hf_layoutlm(path_config_json,\n",
    "                     merge,\n",
    "                     path_weights,\n",
    "                     config_overwrite=[\"max_steps=2000\",\n",
    "                                       \"per_device_train_batch_size=8\",\n",
    "                                       \"eval_steps=100\",\n",
    "                                       \"save_steps=400\",\n",
    "                                       \"use_wandb=True\",\n",
    "                                       \"wandb_project=FRFPE_layoutlmv2\"],\n",
    "                     log_dir=\"/path/to/dir/Experiments/FRFPE/layoutlmv2\",\n",
    "                     dataset_val=merge,\n",
    "                     metric=metric,\n",
    "                     use_token_tag=False,\n",
    "                     pipeline_component_name=\"LMTokenClassifierService\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluation metrics show that the first checkpoint already delivers one of the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[0608 15:54.11 @eval.py:113]\u001b[0m  \u001b[32mINF\u001b[0m  \u001b[97mBuilding multi threading pipeline component to increase prediction throughput. Using 2 threads\u001b[0m\n",
      "\u001b[32m[0608 15:54.14 @eval.py:225]\u001b[0m  \u001b[32mINF\u001b[0m  \u001b[97mPredicting objects...\u001b[0m\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26/26 [00:01<00:00, 17.09it/s]\n",
      "\u001b[32m[0608 15:54.15 @eval.py:207]\u001b[0m  \u001b[32mINF\u001b[0m  \u001b[97mStarting evaluation...\u001b[0m\n",
      "\u001b[32m[0608 15:54.15 @accmetric.py:373]\u001b[0m  \u001b[32mINF\u001b[0m  \u001b[97mF1 results:\n",
      " \u001b[36m|     key     | category_id   | val      | num_samples   |\n",
      "|:-----------:|:--------------|:---------|:--------------|\n",
      "|    word     | 1             | 1        | 1505          |\n",
      "| token_class | 1             | 0.950276 | 89            |\n",
      "| token_class | 2             | 0.790323 | 69            |\n",
      "| token_class | 3             | 0.688312 | 86            |\n",
      "| token_class | 4             | 0.858974 | 490           |\n",
      "| token_class | 5             | 0.90031  | 771           |\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "categories = ner.dataflow.categories.get_sub_categories(categories=\"word\",\n",
    "                                                        sub_categories={\"word\": [\"token_class\"]},\n",
    "                                                        keys=False)[\"word\"][\"token_class\"]\n",
    "\n",
    "path_config_json = \"/path/to/dir/Experiments/FRFPE/layoutlmv2/checkpoint-400/config.json\"\n",
    "path_weights = \"/path/to/dir/Experiments/FRFPE/layoutlmv2/checkpoint-400/pytorch_model.bin\"\n",
    "\n",
    "layoutlm_classifier = dd.HFLayoutLmv2TokenClassifier(path_config_json,\n",
    "                                                   path_weights,\n",
    "                                                   categories=categories)\n",
    "\n",
    "tokenizer_fast = LayoutLMTokenizerFast.from_pretrained(\"microsoft/layoutlm-base-uncased\")  # tokenizer is the same as for LayoutLMv1\n",
    "pipe_component = dd.LMTokenClassifierService(tokenizer_fast,\n",
    "                                             layoutlm_classifier,\n",
    "                                             use_other_as_default_category=True)\n",
    "\n",
    "evaluator = dd.Evaluator(merge, pipe_component, metric)\n",
    "_ = evaluator.run(split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is little to no improvement compared with LayouLMv1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[0608 16:09.51 @eval.py:113]\u001b[0m  \u001b[32mINF\u001b[0m  \u001b[97mBuilding multi threading pipeline component to increase prediction throughput. Using 2 threads\u001b[0m\n",
      "\u001b[32m[0608 16:09.54 @eval.py:225]\u001b[0m  \u001b[32mINF\u001b[0m  \u001b[97mPredicting objects...\u001b[0m\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26/26 [00:01<00:00, 17.40it/s]\n",
      "\u001b[32m[0608 16:09.55 @eval.py:207]\u001b[0m  \u001b[32mINF\u001b[0m  \u001b[97mStarting evaluation...\u001b[0m\n",
      "\u001b[32m[0608 16:09.55 @accmetric.py:431]\u001b[0m  \u001b[32mINF\u001b[0m  \u001b[97mConfusion matrix: \n",
      " \u001b[36m|    predictions ->  |   1 |   2 |   3 |   4 |   5 |\n",
      "|     ground truth | |     |     |     |     |     |\n",
      "|                  v |     |     |     |     |     |\n",
      "|-------------------:|----:|----:|----:|----:|----:|\n",
      "|                  1 |  86 |   0 |   0 |   0 |   3 |\n",
      "|                  2 |   0 |  49 |   0 |   0 |  20 |\n",
      "|                  3 |   0 |   0 |  53 |  12 |  21 |\n",
      "|                  4 |   0 |   0 |  15 | 402 |  73 |\n",
      "|                  5 |   6 |   6 |   0 |  32 | 727 |\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "metric = dd.get_metric(\"confusion\")\n",
    "metric.set_categories(sub_category_names={\"word\": [\"token_class\"]})\n",
    "\n",
    "evaluator = dd.Evaluator(merge, pipe_component, metric)\n",
    "_ = evaluator.run(split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LayoutXLM for financial report NER\n",
    "\n",
    "Next, we turn our attention to LayoutXLM which is a multi-language model. The training setting in the first experiment will be unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_config_json = dd.ModelCatalog.get_full_path_configs(\"microsoft/layoutxlm-base/pytorch_model.bin\")\n",
    "path_weights = dd.ModelCatalog.get_full_path_weights(\"microsoft/layoutxlm-base/pytorch_model.bin\")\n",
    "\n",
    "metric = dd.get_metric(\"f1\")\n",
    "metric.set_categories(sub_category_names={\"word\": [\"token_class\"]})\n",
    "\n",
    "dd.train_hf_layoutlm(path_config_json,\n",
    "                     merge,\n",
    "                     path_weights,\n",
    "                     config_overwrite=[\"max_steps=2000\",\n",
    "                                       \"per_device_train_batch_size=8\",\n",
    "                                       \"eval_steps=100\",\n",
    "                                       \"save_steps=400\",\n",
    "                                       \"use_wandb=True\",\n",
    "                                       \"wandb_project=FRFPE_layoutxlm\"],\n",
    "                         log_dir=\"/path/to/dir/Experiments/FRFPE/layoutxlm\",\n",
    "                         dataset_val=merge,\n",
    "                         metric=metric,\n",
    "                         use_xlm_tokenizer=True, # layoutv2 are layoutlm are from layer perspective identical. However, they do not share the same tokenizer. We therefore need to provide the information to the training script.\n",
    "                         use_token_tag=False,\n",
    "                         pipeline_component_name=\"LMTokenClassifierService\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalutation result look a lot more promising. We get an F1-score close to 0.9 along all labels.  \n",
    "\n",
    "This is backed by the very impressive F1-results on the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LayoutXLMTokenizer'. \n",
      "The class this function is called from is 'XLMRobertaTokenizerFast'.\n",
      "\u001b[32m[0608 16:58.17 @eval.py:113]\u001b[0m  \u001b[32mINF\u001b[0m  \u001b[97mBuilding multi threading pipeline component to increase prediction throughput. Using 2 threads\u001b[0m\n",
      "\u001b[32m[0608 16:58.20 @eval.py:225]\u001b[0m  \u001b[32mINF\u001b[0m  \u001b[97mPredicting objects...\u001b[0m\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26/26 [00:01<00:00, 18.60it/s]\n",
      "\u001b[32m[0608 16:58.22 @eval.py:207]\u001b[0m  \u001b[32mINF\u001b[0m  \u001b[97mStarting evaluation...\u001b[0m\n",
      "\u001b[32m[0608 16:58.22 @accmetric.py:373]\u001b[0m  \u001b[32mINF\u001b[0m  \u001b[97mF1 results:\n",
      " \u001b[36m|     key     | category_id   | val      | num_samples   |\n",
      "|:-----------:|:--------------|:---------|:--------------|\n",
      "|    word     | 1             | 1        | 1505          |\n",
      "| token_class | 1             | 0.949721 | 89            |\n",
      "| token_class | 2             | 0.930556 | 69            |\n",
      "| token_class | 3             | 0.911111 | 86            |\n",
      "| token_class | 4             | 0.960825 | 490           |\n",
      "| token_class | 5             | 0.970722 | 771           |\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "categories = ner.dataflow.categories.get_sub_categories(categories=\"word\",\n",
    "                                                        sub_categories={\"word\": [\"token_class\"]},\n",
    "                                                        keys=False)[\"word\"][\"token_class\"]\n",
    "\n",
    "path_config_json = \"/path/to/dir/Experiments/FRFPE/layoutxlm/checkpoint-1600/config.json\"\n",
    "path_weights = \"/path/to/dir/Experiments/FRFPE/layoutxlm/checkpoint-1600/pytorch_model.bin\"\n",
    "\n",
    "layoutlm_classifier = dd.HFLayoutLmv2TokenClassifier(path_config_json,\n",
    "                                                     path_weights,\n",
    "                                                     categories=categories,\n",
    "                                                     use_xlm_tokenizer=True)\n",
    "\n",
    "tokenizer_fast = XLMRobertaTokenizerFast.from_pretrained(\"microsoft/layoutxlm-base\")\n",
    "tokenizer_fast.model_max_length=512 # Instantiating the tokenizer the way we do above seems to be problematic as \n",
    "                                    # no max_length is provided and the tokenizer therefore does not truncate the\n",
    "                                    # sequence. We therefore have to set this value manually.\n",
    "\n",
    "pipe_component = dd.LMTokenClassifierService(tokenizer_fast,\n",
    "                                             layoutlm_classifier,\n",
    "                                             use_other_as_default_category=True)\n",
    "\n",
    "evaluator = dd.Evaluator(merge, pipe_component, metric)\n",
    "_ = evaluator.run(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.compare(interactive=True, split=\"test\", show_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training XLM models on separate languages\n",
    "\n",
    "Of course, there are various experimentation options here as well. For example, one could investigate whether one gets better results when training XLM models for each language separately. In our case, one could train one each on English and German data (there are too few data points for a French model). \n",
    "\n",
    "For this, one would have to filter the data set once for English and German data points. E.g. training a german model would look like this:\n",
    "\n",
    "```\n",
    "df = ner.dataflow.build(load_image=True, filter_languages=[dd.Languages.german])\n",
    "\n",
    "merge = dd.MergeDataset(ner)\n",
    "merge.explicit_dataflows(df)\n",
    "merge.buffer_datasets()\n",
    "merge.create_split_by_id(split_dict)\n",
    "\n",
    "path_config_json = dd.ModelCatalog.get_full_path_configs(\"microsoft/layoutxlm-base/pytorch_model.bin\")\n",
    "path_weights = dd.ModelCatalog.get_full_path_weights(\"microsoft/layoutxlm-base/pytorch_model.bin\")\n",
    "\n",
    "metric = dd.get_metric(\"f1\")\n",
    "metric.set_categories(sub_category_names={\"word\": [\"token_class\"]})\n",
    "\n",
    "dd.train_hf_layoutlm(path_config_json,\n",
    "                     merge,\n",
    "                     path_weights,\n",
    "                     config_overwrite=[\"max_steps=2000\",\n",
    "                                       \"per_device_train_batch_size=8\",\n",
    "                                       \"eval_steps=100\",\n",
    "                                       \"save_steps=400\",\n",
    "                                       \"use_wandb=True\",\n",
    "                                       \"wandb_project=FRFPE_layoutxlm\"],\n",
    "                         log_dir=\"/path/to/dir/Experiments/FRFPE/layoutxlm\",\n",
    "                         dataset_val=merge,\n",
    "                         metric=metric,\n",
    "                         use_xlm_tokenizer=True, \n",
    "                         use_token_tag=False,\n",
    "                         pipeline_component_name=\"LMTokenClassifierService\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
